{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import config  # Import your config.py file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import time\n",
    "import re\n",
    "# Set up the OpenAI API key from the config.py file\n",
    "openai.api_key = config.api_key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "def get_model_response(prompt, engine=\"gpt-3.5-turbo\"):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"This is the system message\"},\n",
    "        {\"role\": \"user\", \"content\":prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=engine,\n",
    "        messages=messages,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.2,\n",
    "        top_p = 0.9,\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "def identify_gics_classes(df, class_list):\n",
    "\n",
    "    temp_df = df.copy()\n",
    "    temp_df['classes'] = \"\"\n",
    "    for i in range(0, temp_df.shape[0]):\n",
    "\n",
    "        biography = temp_df.at[i,'Biography']\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "                    Read the list of Global Industry Classification Standard  (GICS) industry classes shown surrounded by 4 colon's below\n",
    "                    ::::\n",
    "                    {class_list}\n",
    "                    ::::\n",
    "\n",
    "                    Now read the the biography below surrounded by 3 colons, return a python list of GICS classes that most appropriately match the text\n",
    "                    :::\n",
    "                    {biography}\n",
    "                    :::\n",
    "                    the return string should be in the form shown below and should contain at least 1 entry\n",
    "                    [entry_1, entry_2,.. entry_n]\n",
    "                    \"\"\"\n",
    "\n",
    "        temp_df.loc[i, 'classes'] = get_model_response(prompt, engine=\"gpt-3.5-turbo\")\n",
    "\n",
    "    return temp_df\n",
    "##\n",
    "## has timer to prevent exceeding limit\n",
    "##\n",
    "def identify_gics_classes(df, class_list, rate_limit=60000):\n",
    "    temp_df = df.copy()\n",
    "    temp_df['classes'] = \"\"\n",
    "    tokens_accumulated = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, temp_df.shape[0]):\n",
    "\n",
    "        biography = temp_df.at[i,'Biography']\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "                    Read the list of Global Industry Classification Standard  (GICS) industry classes shown surrounded by 4 colon's below\n",
    "                    ::::\n",
    "                    {class_list}\n",
    "                    ::::\n",
    "\n",
    "                    Now read the the biography below surrounded by 3 colons, return a python list of GICS classes that most appropriately match the text\n",
    "                    :::\n",
    "                    {biography}\n",
    "                    :::\n",
    "                    the return string should be in the form shown below and should contain at least 1 entry\n",
    "                    [entry_1, entry_2,.. entry_n]\n",
    "                    \"\"\"\n",
    "\n",
    "        classes, tokens = get_model_response(prompt, engine=\"gpt-3.5-turbo\")\n",
    "        temp_df.loc[i, 'classes'] = classes\n",
    "        tokens_accumulated += tokens\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if tokens_accumulated / elapsed_time > rate_limit/60:  # tokens per second\n",
    "            sleep_time = tokens_accumulated/rate_limit - elapsed_time\n",
    "            time.sleep(sleep_time)  # sleep if rate limit is reached\n",
    "\n",
    "    return temp_df\n",
    "\n",
    "    \n",
    "path_folder = './data/cities_json/London'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(path_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './data/cities_json'\n",
    "bios_path = './data/all_bios_embeddings.csv'\n",
    "missing_bios_path = './data/missing_bios.csv'\n",
    "if os.path.exists(bios_path):\n",
    "    # Load the existing CSV if it exists\n",
    "    all_bios_df = pd.read_csv(bios_path)\n",
    "    #this conversion takes about 10 secs. a better storage method may be preferable aka, index = file name columns are numeric embeddings\n",
    "    all_bios_df['embedding'] = all_bios_df['embedding'].apply(ast.literal_eval)\n",
    "    missing_bio_df = pd.read_csv(missing_bios_path)\n",
    "else:\n",
    "    # Run the code to generate DataFrames a and b\n",
    "    all_bios_df = []\n",
    "    for path in os.listdir(data_root ):\n",
    "\n",
    "        all_files = os.listdir(data_root +'/' + path)\n",
    "\n",
    "        for file in all_files:\n",
    "\n",
    "            temp_dict = load_pickle_file(data_root +'/' + path +'/'+file)\n",
    "\n",
    "            if 'biography' not in temp_dict:\n",
    "                temp_dict['biography'] = pd.DataFrame({'Biography':[\"\"]})\n",
    "            \n",
    "            temp_df = temp_dict['biography']\n",
    "            temp_df['city'] = path\n",
    "            temp_df['file'] = file\n",
    "            temp_df['file'] = temp_df['file'].replace('.pkl', \"\")\n",
    "\n",
    "            all_bios_df.append(temp_df)\n",
    "\n",
    "\n",
    "    all_bios_df = pd.concat(all_bios_df, ignore_index=True)\n",
    "    all_bios_df = all_bios_df[['file', 'city','Biography']]\n",
    "\n",
    "    all_bios_df['no_bio']=(all_bios_df['Biography'].apply(len)<50)\n",
    "    missing_bio_df = all_bios_df.loc[all_bios_df['no_bio'] ].copy().drop('no_bio', axis = 1)\n",
    "\n",
    "    all_bios_df = all_bios_df.loc[~all_bios_df['no_bio'] ].drop('no_bio', axis = 1)\n",
    "\n",
    "    #this takes about 12 minutes and costs money, only run when necessary!!!\n",
    "    all_bios_df['embedding'] = all_bios_df['Biography'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "    missing_bio_df.to_csv(missing_bios_path, index = False)\n",
    "    all_bios_df.to_csv(bios_path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "Frankfurt                 103\n",
       "Hong Kong                 384\n",
       "Johannesburg              124\n",
       "Lagos                     246\n",
       "London                    394\n",
       "Mexico City               104\n",
       "New York                  389\n",
       "Rio de Janeiro             52\n",
       "San Francisco Bay Area    390\n",
       "Sydney                    316\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bios_df.groupby(['city']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gics_embeddings_path = './data/gics_embeddings.csv'\n",
    "if os.path.exists(gics_embeddings_path):\n",
    "    # Load the existing CSV if it exists\n",
    "    gics_embeddings_df = pd.read_csv(gics_embeddings_path)\n",
    "    gics_embeddings_df = gics_embeddings_df['embedding'].apply(ast.literal_eval)\n",
    "\n",
    "else:\n",
    "    gics_embeddings_df = pd.read_csv('./data/gics-map-2018.csv')\n",
    "    gics_embeddings_df['embedding_desc'] = gics_embeddings_df['SubIndustryDescription'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "    gics_embeddings_df['embedding'] = gics_embeddings_df['SubIndustry'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "    gics_embeddings_df['token_count'] = gics_embeddings_df['SubIndustryDescription'].apply(lambda x: len(encoding.encode(x)))\n",
    "    gics_embeddings_df.to_csv(gics_embeddings_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform auto classification of the gics classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-gpt-3.5-turbo in organization org-PXtP9xMXWP8KqzH24Bz03Cgi on tokens per min. Limit: 90000 / min. Current: 87745 / min. Contact us through our help center at help.openai.com if you continue to have issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#prevents the calls to gpt being re-run unnecessarily\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mclasses\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m all_bios_df\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m----> 3\u001b[0m     all_bios_df \u001b[39m=\u001b[39m identify_gics_classes(all_bios_df, \u001b[39mstr\u001b[39;49m(gics_embeddings_df[\u001b[39m'\u001b[39;49m\u001b[39mIndustry\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49munique()\u001b[39m.\u001b[39;49mtolist()))\n\u001b[1;32m      4\u001b[0m     all_bios_df\u001b[39m.\u001b[39mto_csv(bios_path, index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36midentify_gics_classes\u001b[0;34m(df, class_list)\u001b[0m\n\u001b[1;32m     33\u001b[0m     biography \u001b[39m=\u001b[39m temp_df\u001b[39m.\u001b[39mat[i,\u001b[39m'\u001b[39m\u001b[39mBiography\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m                Read the list of Global Industry Classification Standard  (GICS) industry classes shown surrounded by 4 colon\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms below\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m                [entry_1, entry_2,.. entry_n]\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[39m                \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 50\u001b[0m     temp_df\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39mclasses\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m get_model_response(prompt, engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m temp_df\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mget_model_response\u001b[0;34m(prompt, engine)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model_response\u001b[39m(prompt, engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     12\u001b[0m     messages \u001b[39m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mThis is the system message\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     14\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m:prompt}\n\u001b[1;32m     15\u001b[0m     ]\n\u001b[0;32m---> 17\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     18\u001b[0m         model\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m     19\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     20\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m         temperature \u001b[39m=\u001b[39;49m \u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m         top_p \u001b[39m=\u001b[39;49m \u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-PXtP9xMXWP8KqzH24Bz03Cgi on tokens per min. Limit: 90000 / min. Current: 87745 / min. Contact us through our help center at help.openai.com if you continue to have issues."
     ]
    }
   ],
   "source": [
    "#prevents the calls to gpt being re-run unnecessarily\n",
    "if 'classes' not in all_bios_df.columns:\n",
    "    all_bios_df = identify_gics_classes(all_bios_df, str(gics_embeddings_df['Industry'].unique().tolist()))\n",
    "    all_bios_df.to_csv(bios_path, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a binary multi-label dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'classes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Iterate through each class in the original list and construct the binary data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m class_name \u001b[39min\u001b[39;00m gics_embeddings_df[\u001b[39m'\u001b[39m\u001b[39mIndustry\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39mtolist():\n\u001b[0;32m----> 5\u001b[0m     binary_data[class_name] \u001b[39m=\u001b[39m all_bios_df[\u001b[39m'\u001b[39;49m\u001b[39mclasses\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m class_name \u001b[39min\u001b[39;00m x \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m binary_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(binary_data)\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/wealthnets/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'classes'"
     ]
    }
   ],
   "source": [
    "binary_data = {}\n",
    "\n",
    "# Iterate through each class in the original list and construct the binary data\n",
    "for class_name in gics_embeddings_df['Industry'].unique().tolist():\n",
    "    binary_data[class_name] = all_bios_df['classes'].apply(lambda x: 1 if class_name in x else 0)\n",
    "\n",
    "binary_df = pd.DataFrame(binary_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify ethnicities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './data/cities_json'\n",
    "names_path = './data/names_ethnicity.csv'\n",
    "pattern = r\"Wealth-X(.*?)Dossier\"\n",
    "\n",
    "if os.path.exists(names_path):\n",
    "    # Load the existing CSV if it exists\n",
    "    names_ethnicity_df = pd.read_csv(names_path)\n",
    "\n",
    "else:\n",
    "    # Run the code to generate DataFrames\n",
    "    names_ethnicity_df = []\n",
    "    \n",
    "    for path in os.listdir(data_root):\n",
    "        full_path = f\"{data_root}/{path}\"\n",
    "        all_files = os.listdir(full_path)\n",
    "\n",
    "        temp_df = pd.DataFrame(all_files, columns=['file'])\n",
    "        temp_df['city'] = path\n",
    "        temp_df['file'] = temp_df['file'].str.replace('.pkl', \"\", regex=False)\n",
    "        temp_df['name'] = temp_df['file'].str.extract(pattern)\n",
    "        temp_df['name'] = temp_df['name'].str.lower() \n",
    "\n",
    "        names_ethnicity_df.append(temp_df)\n",
    "\n",
    "    names_ethnicity_df = pd.concat(names_ethnicity_df, ignore_index=True)\n",
    "    names_ethnicity_df = names_ethnicity_df[['file','city', 'name' ]]\n",
    "    names_ethnicity_df['name_city'] = [* zip(names_ethnicity_df['name'], names_ethnicity_df['city'])]\n",
    "    names_ethnicity_df['name_city'] = names_ethnicity_df['name_city'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>city</th>\n",
       "      <th>name</th>\n",
       "      <th>name_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L265 Wealth-X Kola Edward EDGAL Dossier</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>kola edward edgal</td>\n",
       "      <td>( kola edward edgal , Lagos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L195 Wealth-X Victor Gbolade OSIBODU Dossier</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>victor gbolade osibodu</td>\n",
       "      <td>( victor gbolade osibodu , Lagos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L256 Wealth-X Harrison Eyitayo ILORI Dossier</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>harrison eyitayo ilori</td>\n",
       "      <td>( harrison eyitayo ilori , Lagos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L145 Wealth-X Deji  ALLI Dossier</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>deji  alli</td>\n",
       "      <td>( deji  alli , Lagos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L135 Wealth-X Ishwardas  MAHTANI Dossier</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>ishwardas  mahtani</td>\n",
       "      <td>( ishwardas  mahtani , Lagos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>N341 Wealth-X Robert Jeffrey SPEYER Dossier</td>\n",
       "      <td>New York</td>\n",
       "      <td>robert jeffrey speyer</td>\n",
       "      <td>( robert jeffrey speyer , New York)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>N276 Wealth-X Madonna Louise Veronica CICCONE ...</td>\n",
       "      <td>New York</td>\n",
       "      <td>madonna louise veronica ciccone</td>\n",
       "      <td>( madonna louise veronica ciccone , New York)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>N069 Wealth-X Stephen Alan WYNN Dossier</td>\n",
       "      <td>New York</td>\n",
       "      <td>stephen alan wynn</td>\n",
       "      <td>( stephen alan wynn , New York)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>N133 Wealth-X Fawaz Abdulaziz Fahad ALHOKAIR D...</td>\n",
       "      <td>New York</td>\n",
       "      <td>fawaz abdulaziz fahad alhokair</td>\n",
       "      <td>( fawaz abdulaziz fahad alhokair , New York)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>N024 Wealth-X John Tsung-Chen CHAO Dossier</td>\n",
       "      <td>New York</td>\n",
       "      <td>john tsung-chen chao</td>\n",
       "      <td>( john tsung-chen chao , New York)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2868 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file      city  \\\n",
       "0               L265 Wealth-X Kola Edward EDGAL Dossier     Lagos   \n",
       "1          L195 Wealth-X Victor Gbolade OSIBODU Dossier     Lagos   \n",
       "2          L256 Wealth-X Harrison Eyitayo ILORI Dossier     Lagos   \n",
       "3                      L145 Wealth-X Deji  ALLI Dossier     Lagos   \n",
       "4              L135 Wealth-X Ishwardas  MAHTANI Dossier     Lagos   \n",
       "...                                                 ...       ...   \n",
       "2863        N341 Wealth-X Robert Jeffrey SPEYER Dossier  New York   \n",
       "2864  N276 Wealth-X Madonna Louise Veronica CICCONE ...  New York   \n",
       "2865            N069 Wealth-X Stephen Alan WYNN Dossier  New York   \n",
       "2866  N133 Wealth-X Fawaz Abdulaziz Fahad ALHOKAIR D...  New York   \n",
       "2867         N024 Wealth-X John Tsung-Chen CHAO Dossier  New York   \n",
       "\n",
       "                                   name  \\\n",
       "0                    kola edward edgal    \n",
       "1               victor gbolade osibodu    \n",
       "2               harrison eyitayo ilori    \n",
       "3                           deji  alli    \n",
       "4                   ishwardas  mahtani    \n",
       "...                                 ...   \n",
       "2863             robert jeffrey speyer    \n",
       "2864   madonna louise veronica ciccone    \n",
       "2865                 stephen alan wynn    \n",
       "2866    fawaz abdulaziz fahad alhokair    \n",
       "2867              john tsung-chen chao    \n",
       "\n",
       "                                          name_city  \n",
       "0                      ( kola edward edgal , Lagos)  \n",
       "1                 ( victor gbolade osibodu , Lagos)  \n",
       "2                 ( harrison eyitayo ilori , Lagos)  \n",
       "3                             ( deji  alli , Lagos)  \n",
       "4                     ( ishwardas  mahtani , Lagos)  \n",
       "...                                             ...  \n",
       "2863            ( robert jeffrey speyer , New York)  \n",
       "2864  ( madonna louise veronica ciccone , New York)  \n",
       "2865                ( stephen alan wynn , New York)  \n",
       "2866   ( fawaz abdulaziz fahad alhokair , New York)  \n",
       "2867             ( john tsung-chen chao , New York)  \n",
       "\n",
       "[2868 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_ethnicity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_dict = {\n",
    "    1: \"Indian Subcontinent\",\n",
    "    2: \"Russia\",\n",
    "    3: \"China\",\n",
    "    4: \"Anglophone (United States, United Kingdom, Australia, Canada, etc.)\",\n",
    "    5: \"Germany\",\n",
    "    6: \"Brazil\",\n",
    "    7: \"Middle East and North Africa (MENA)\",\n",
    "    8: \"Sub-Saharan Africa\",\n",
    "    9: \"Latin America (excluding Brazil)\",\n",
    "    10: \"East Asia (excluding China)\",\n",
    "    11: \"South East Asia\",\n",
    "    12: \"Eastern Europe (excluding Russia)\",\n",
    "    13: \"Western Europe (excluding Germany and the Anglophone countries)\"\n",
    "}\n",
    "\n",
    "def get_model_response2(prompt, engine=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"This is the system message\"},\n",
    "        {\"role\": \"user\", \"content\":prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=engine,\n",
    "        messages=messages,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.2,\n",
    "        top_p = 0.9,\n",
    "    )\n",
    "    \n",
    "    #tokens_used = response['usage']['total_tokens']\n",
    "\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "def identify_ethncity_classes(df, class_list, chunk_size = 100 , rate_limit=60000):\n",
    "    temp_df = df.copy()\n",
    "    temp_df['classes'] = \"\"\n",
    "    tokens_accumulated = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Empty list to store processed chunks\n",
    "    processed_chunks = []\n",
    "\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = temp_df.iloc[i:i + chunk_size]  # Get the current chunk\n",
    "\n",
    "        pairs = chunk.loc[:,'name_city']\n",
    "        pairs_string = '\\n'.join([f'{p[0]} from {p[1]}' for p in pairs])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "                    Read the list of country of origin industry classes shown surrounded by 4 colon's below\n",
    "                    ::::\n",
    "                    {class_list}\n",
    "                    ::::\n",
    "\n",
    "                    Each element of the list below contains a name and city of residence pair, the most appropriate class\n",
    "                    for each element of the list needs to be provided\n",
    "                    :::\n",
    "                    {pairs_string }\n",
    "                    :::\n",
    "                    the return string should be in the form shown below\n",
    "                    [entry_1, entry_2,.. entry_n]\n",
    "                    \"\"\"\n",
    "\n",
    "        classes, tokens = get_model_response(prompt, engine=\"gpt-3.5-turbo\")\n",
    "        chunk.loc[:, 'classes'] = classes\n",
    "        tokens_accumulated += tokens\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if tokens_accumulated / elapsed_time > rate_limit/60:  # tokens per second\n",
    "            sleep_time = tokens_accumulated/rate_limit - elapsed_time\n",
    "            time.sleep(sleep_time)  # sleep if rate limit is reached\n",
    "\n",
    "        processed_chunks.append(chunk)\n",
    "\n",
    "    return pd.concat(processed_chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "            You are an expert in the regional origin of names ::: Read the python dictionary of country of origin ethnicity classes shown surrounded by 4 colon's below. the dictionary is a class id: description pair\n",
    "            ::::{class_list}::::\n",
    "\n",
    "            Each element of the list below contains a tuple of (person name, city of residence pair), consider their name and city of residence then\n",
    "              choose most appropriate ethnicity class id, \n",
    "            from the previous dictionary for each element of the list needs to be provided\n",
    "            :::{pairs}:::\n",
    "            the return string should be in the form shown below\n",
    "            [entry_1, entry_2,.. entry_n]\n",
    "            \"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "            You are an expert in the regional origin of names :::  Read the python dictionary of country of origin ethnicity classes shown surrounded by 4 colon's below. the dictionary is a class id: description pair\n",
    "            ::::{class_list}::::\n",
    "\n",
    "            Each element of the list below contains a persons name  choose most appropriate ethnicity class id, \n",
    "            from the previous dictionary for each element of the list needs to be provided\n",
    "            :::{pairs}:::\n",
    "            the return string should be in the form shown below\n",
    "            [entry_1, entry_2,.. entry_n]\n",
    "            \"\"\"\n",
    "\n",
    "prompt = \"\"\"You are an expert in the regional origin of names ::: what is the origin of the following names, with particular consideration of the last name [' kola edward edgal ',\n",
    " ' victor gbolade osibodu ',\n",
    " ' harrison eyitayo ilori ',\n",
    " ' deji  alli ',\n",
    " ' ishwardas  mahtani ',\n",
    " ' segun  fagboyegun ',\n",
    " ' rajan vashdev vaswani ',\n",
    " ' gabriel g. boulos ',\n",
    " ' olalekan akinsoga akinyanmi ',\n",
    " ' ajoritsedere josephine awosika ',\n",
    " ' oladipo  odujinrin ']\n",
    "\n",
    "please select the origin from one of the below categories\n",
    "\n",
    "{1: 'Indian Subcontinent',\n",
    " 2: 'Russia',\n",
    " 3: 'China',\n",
    " 4: 'Anglophone (United States, United Kingdom, Australia, Canada, etc.)',\n",
    " 5: 'Germany',\n",
    " 6: 'Brazil',\n",
    " 7: 'Middle East and North Africa (MENA)',\n",
    " 8: 'Sub-Saharan Africa',\n",
    " 9: 'Latin America (excluding Brazil)',\n",
    " 10: 'East Asia (excluding China)',\n",
    " 11: 'South East Asia',\n",
    " 12: 'Eastern Europe (excluding Russia)',\n",
    " 13: 'Western Europe (excluding Germany and the Anglophone countries)'}\n",
    "\n",
    "You response should be in the form [cat_number1, cat_number2, ...,cat_numbern]\n",
    "for example a two element list = ['david cameron', 'sadiq khan']\n",
    "    returns a two element response = [4,1]\"\"\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"You are an expert in the regional origin of names ::: what is the origin of the following names, with particular consideration of the last name {pairs}\n",
    "\n",
    "    please select the origin from one of the below categories\n",
    "\n",
    "    {class_list}\n",
    "\n",
    "    You response should be in the form [cat_number1, cat_number2, ...,cat_numbern]\n",
    "    for example a two element list = ['david cameron', 'sadiq khan', sim sang-jung,'goodluck jonathan', 'angela merkle']\n",
    "        returns a two element response = [4,1, 10, 8, 4]\"\"\"\n",
    "\n",
    "prompt = f\"\"\" You are an expert in the regional origin of names ::: read the dictionary of names classes {class_list}\n",
    "    now look at the question answer format below\n",
    "    question :['david cameron', 'sadiq khan', sim sang-jung,'goodluck jonathan', 'angela merkle']\n",
    "    answer:[4,1, 10, 8, 4]\n",
    "    \n",
    "    respond appropriately\n",
    "    question:{pairs}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "name_origin_json  =     {\n",
    "  \"david cameron\": 4,\n",
    "  \"sadiq khan\": 1,\n",
    "  \"sim sang-jung\": 10,\n",
    "  \"goodluck jonathan\": 8,\n",
    "  \"angela merkle\": 4\n",
    "}\n",
    "prompt = f\"\"\" You are an expert in the regional origin of names ::: read the dictionary of names classes {class_list}\n",
    "    \n",
    "    now see the example name-origin json\n",
    "    {name_origin_json}\n",
    "\n",
    "    \n",
    "    take the list below and return it in the same format as the example\n",
    "    {pairs}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "f\"\"\"You are an expert in the regional origin of names. read the dictionary of ethnic origin classes {class_list}\n",
    "    \n",
    "    now see the example name-origin json, the names have been paired with the most appropriate ethnic/country origin\n",
    "    {name_origin_json}\n",
    "\n",
    "    what is the origin of the following names, with particular consideration of the last name, and the fact that all the people live in the city of {city}\n",
    "    take the list below, wcomplete the origin paying particular consideration of the last name,  and return it in the same format as the example\n",
    "    {pairs}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "system_message = f\"\"\"You are an expert in the regional origin of names. Read the dictionary of ethnic origin classes (eth_dict) surrounded by triple colons \n",
    ":::{countries_dict}:::\n",
    "    \n",
    "    now see the example name-origin json, the names have been paired with the most appropriate ethnic/country origin from the eth_dict\n",
    "    {name_origin_json}\n",
    "\n",
    "    The user will supply you with a json of peoples names (names_dict) and the city that the people live in. using this information you must reason through the most,\n",
    "    likely ethnic/regional origin of the name from the list, paying particular attention the last name. \n",
    "    Construct your response to have the same format as the example json, however only return the list of extracted values.\n",
    "    as such your response will have the form\n",
    "    [value1, value2,...valuen]\n",
    "    check that the number of entries is equal to the number of names supplied, if the numbers are not equal find the error and correct, then return the list\n",
    "    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>city</th>\n",
       "      <th>name</th>\n",
       "      <th>name_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>Wealth-X Hamad bin Khalifa bin Hamad bin Abdul...</td>\n",
       "      <td>London</td>\n",
       "      <td>hamad bin khalifa bin hamad bin abdullah bin ...</td>\n",
       "      <td>(' hamad bin khalifa bin hamad bin abdullah bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>Wealth-X John Lionel Beckwith Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>john lionel beckwith</td>\n",
       "      <td>(' john lionel beckwith ', 'London')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>Wealth-X Kristo Kaarmann Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>kristo kaarmann</td>\n",
       "      <td>(' kristo kaarmann ', 'London')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>Wealth-X Maritsa Lazari Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>maritsa lazari</td>\n",
       "      <td>(' maritsa lazari ', 'London')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>Wealth-X Andreas Serenus Hoffmann Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>andreas serenus hoffmann</td>\n",
       "      <td>(' andreas serenus hoffmann ', 'London')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>Wealth-X Carl-Henric Svanberg Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>carl-henric svanberg</td>\n",
       "      <td>(' carl-henric svanberg ', 'London')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>Wealth-X Fred Done Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>fred done</td>\n",
       "      <td>(' fred done ', 'London')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>Wealth-X Mark Andrew Pears Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>mark andrew pears</td>\n",
       "      <td>(' mark andrew pears ', 'London')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>Wealth-X Magdi Abdul Latif Jameel Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>magdi abdul latif jameel</td>\n",
       "      <td>(' magdi abdul latif jameel ', 'London')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>Wealth-X Troels Holch Povlsen Dossier</td>\n",
       "      <td>London</td>\n",
       "      <td>troels holch povlsen</td>\n",
       "      <td>(' troels holch povlsen ', 'London')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file    city  \\\n",
       "1685  Wealth-X Hamad bin Khalifa bin Hamad bin Abdul...  London   \n",
       "1686              Wealth-X John Lionel Beckwith Dossier  London   \n",
       "1687                   Wealth-X Kristo Kaarmann Dossier  London   \n",
       "1688                    Wealth-X Maritsa Lazari Dossier  London   \n",
       "1689          Wealth-X Andreas Serenus Hoffmann Dossier  London   \n",
       "1690              Wealth-X Carl-Henric Svanberg Dossier  London   \n",
       "1691                         Wealth-X Fred Done Dossier  London   \n",
       "1692                 Wealth-X Mark Andrew Pears Dossier  London   \n",
       "1693          Wealth-X Magdi Abdul Latif Jameel Dossier  London   \n",
       "1694              Wealth-X Troels Holch Povlsen Dossier  London   \n",
       "\n",
       "                                                   name  \\\n",
       "1685   hamad bin khalifa bin hamad bin abdullah bin ...   \n",
       "1686                              john lionel beckwith    \n",
       "1687                                   kristo kaarmann    \n",
       "1688                                    maritsa lazari    \n",
       "1689                          andreas serenus hoffmann    \n",
       "1690                              carl-henric svanberg    \n",
       "1691                                         fred done    \n",
       "1692                                 mark andrew pears    \n",
       "1693                          magdi abdul latif jameel    \n",
       "1694                              troels holch povlsen    \n",
       "\n",
       "                                              name_city  \n",
       "1685  (' hamad bin khalifa bin hamad bin abdullah bi...  \n",
       "1686               (' john lionel beckwith ', 'London')  \n",
       "1687                    (' kristo kaarmann ', 'London')  \n",
       "1688                     (' maritsa lazari ', 'London')  \n",
       "1689           (' andreas serenus hoffmann ', 'London')  \n",
       "1690               (' carl-henric svanberg ', 'London')  \n",
       "1691                          (' fred done ', 'London')  \n",
       "1692                  (' mark andrew pears ', 'London')  \n",
       "1693           (' magdi abdul latif jameel ', 'London')  \n",
       "1694               (' troels holch povlsen ', 'London')  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_ethnicity_df.loc[names_ethnicity_df['city']=='London',:].head(10)#.loc[0:9, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_tokens_per_minute):\n",
    "        self.max_tokens_per_minute = max_tokens_per_minute\n",
    "        self.tokens_deque = deque(maxlen=60) # Holds the tokens generated for the past minute.\n",
    "        self.timestamps_deque = deque(maxlen=60) # Holds the timestamps of when tokens were generated.\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Removing tokens older than 1 minute\n",
    "        while self.timestamps_deque and current_time - self.timestamps_deque[0] > 60:\n",
    "            self.timestamps_deque.popleft()\n",
    "            self.tokens_deque.popleft()\n",
    "\n",
    "        # If the number of tokens is more than the maximum limit,\n",
    "        # pause execution until it comes back down below the threshold\n",
    "        if sum(self.tokens_deque) + tokens > self.max_tokens_per_minute:\n",
    "            sleep_time = 60 - (current_time - self.timestamps_deque[0])\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "            # After sleeping, add the tokens and timestamps to the deque\n",
    "            self.tokens_deque.append(tokens)\n",
    "            self.timestamps_deque.append(current_time + sleep_time)\n",
    "        else:\n",
    "            # If the number of tokens is less than the maximum limit,\n",
    "            # add the tokens and timestamps to the deque\n",
    "            self.tokens_deque.append(tokens)\n",
    "            self.timestamps_deque.append(current_time)\n",
    "\n",
    "    def check_tokens(self, tokens):\n",
    "        # Function to check if adding new tokens would exceed limit, without actually adding them\n",
    "        current_time = time.time()\n",
    "        while self.timestamps_deque and current_time - self.timestamps_deque[0] > 60:\n",
    "            self.timestamps_deque.popleft()\n",
    "            self.tokens_deque.popleft()\n",
    "\n",
    "        return sum(self.tokens_deque) + tokens <= self.max_tokens_per_minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'system_message' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m encoding \u001b[39m=\u001b[39m tiktoken\u001b[39m.\u001b[39mencoding_for_model(\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m system_message_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encoding\u001b[39m.\u001b[39mencode(system_message))\n\u001b[1;32m      4\u001b[0m prompt_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encoding\u001b[39m.\u001b[39mencode(prompt))\n\u001b[1;32m      5\u001b[0m system_message_tokens \u001b[39m+\u001b[39mprompt_length \n",
      "\u001b[0;31mNameError\u001b[0m: name 'system_message' is not defined"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "system_message_tokens = len(encoding.encode(system_message))\n",
    "prompt_length = len(encoding.encode(prompt))\n",
    "system_message_tokens +prompt_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total processed:50\n",
      "total processed:100\n",
      "total processed:150\n",
      "total processed:200\n",
      "total processed:250\n",
      "total processed:300\n",
      "total processed:350\n",
      "total processed:400\n",
      "total processed:450\n",
      "total processed:500\n",
      "total processed:550\n",
      "total processed:600\n",
      "total processed:650\n",
      "total processed:700\n",
      "total processed:750\n",
      "total processed:800\n",
      "total processed:850\n",
      "total processed:900\n",
      "total processed:950\n",
      "total processed:1000\n",
      "total processed:1050\n",
      "total processed:1100\n",
      "total processed:1150\n",
      "total processed:1200\n",
      "total processed:1250\n",
      "total processed:1300\n",
      "total processed:1350\n",
      "total processed:1400\n",
      "total processed:1450\n",
      "total processed:1500\n",
      "total processed:1550\n",
      "total processed:1600\n",
      "total processed:1650\n",
      "total processed:1700\n",
      "total processed:1750\n",
      "total processed:1800\n",
      "total processed:1850\n",
      "total processed:1900\n",
      "total processed:1950\n",
      "total processed:2000\n",
      "total processed:2050\n",
      "total processed:2100\n",
      "total processed:2150\n",
      "total processed:2200\n",
      "total processed:2250\n",
      "total processed:2300\n",
      "total processed:2350\n",
      "total processed:2400\n",
      "total processed:2450\n",
      "total processed:2500\n",
      "total processed:2550\n",
      "total processed:2600\n",
      "total processed:2650\n",
      "total processed:2700\n",
      "total processed:2750\n",
      "total processed:2800\n",
      "total processed:2850\n",
      "total processed:2900\n",
      "total processed:2950\n",
      "total processed:3000\n",
      "total processed:3050\n"
     ]
    }
   ],
   "source": [
    "system_message = f\"\"\"You are an expert in the regional origin of names. Read the dictionary of ethnic origin classes (eth_dict) surrounded by triple colons \n",
    ":::{countries_dict}:::\n",
    "    \n",
    "    now see the example name-origin json, the names have been paired with the most appropriate ethnic/country origin from the eth_dict\n",
    "    {name_origin_json}\n",
    "\n",
    "    The user will supply you with a json of peoples names (names_dict) and the city that the people live in. using this information you must reason through the most,\n",
    "    likely ethnic/regional origin of the name from the list, paying particular attention the last name. \n",
    "    Construct your response to have the same format as the example json\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "def get_model_response_ethn(prompt, system_message,engine=\"gpt-3.5-turbo\"):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\":prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=engine,\n",
    "        messages=messages,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.2,\n",
    "        top_p = 0.9,\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "\n",
    "def get_model_response_ethn(prompt, system_message, rate_limiter, engine=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    attempts = 0\n",
    "    while attempts < 5:\n",
    "        try:\n",
    "            prompt_length = len(prompt)  # assuming encoding.encode(prompt) is equivalent to len(prompt)\n",
    "            tokens = len(system_message) + prompt_length\n",
    "            \n",
    "            # Add tokens to rate limiter and sleep if necessary\n",
    "            rate_limiter.add_tokens(tokens)\n",
    "                \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=engine,\n",
    "                messages=messages,\n",
    "                max_tokens=2000,\n",
    "                temperature=0.2,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "            \n",
    "        except openai.error.RateLimitError as e:\n",
    "            print(f\"RateLimitError encountered: {e}, waiting for a minute...\")\n",
    "            time.sleep(60)  # Wait for a minute before retrying\n",
    "            continue  # Continue with the next iteration of the loop, thereby retrying the request\n",
    "            \n",
    "        except openai.error.APIError as e:\n",
    "            print(f\"APIError encountered: {e}, retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "        except openai.error.TimeoutError as e:\n",
    "            print(f\"TimeoutError encountered: {e}, retrying in 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "            \n",
    "        attempts += 1\n",
    "\n",
    "    print(\"Failed to get model response after multiple attempts.\")\n",
    "    return None\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "system_message_tokens = len(encoding.encode(system_message))\n",
    "\n",
    "chunk_size = 50\n",
    "rate_limit=80000\n",
    "city = 'Lagos'\n",
    "total_number = 0\n",
    "# Empty list to store processed chunks\n",
    "processed_chunks = []\n",
    "rate_limiter = RateLimiter(max_tokens_per_minute=rate_limit) \n",
    "for city in names_ethnicity_df['city'].unique():\n",
    "\n",
    "    temp_df = names_ethnicity_df.loc[names_ethnicity_df['city']==city,:].copy().reset_index()\n",
    "    tokens_accumulated = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    for i in range(0, len(temp_df), chunk_size):\n",
    "        chunk = temp_df.iloc[i:i + chunk_size].copy().reset_index()  # Get the current chunk\n",
    "\n",
    "        pairs = chunk.loc[:,'name'].tolist()\n",
    "\n",
    "        prompt = f\"\"\" names_dict :::{pairs}:::\n",
    "        city :::{city}:::\n",
    "        \"\"\"\n",
    "        #prompt_length = len(encoding.encode(prompt))\n",
    "\n",
    "        #rate_limiter.add_tokens(system_message_tokens +prompt_length )\n",
    "\n",
    "        classes = get_model_response_ethn(prompt, system_message, rate_limiter, engine=\"gpt-3.5-turbo\")\n",
    "        #chunk[ 'classes'] = ast.literal_eval(classes)\n",
    "        #chunk.loc[:,'classes'] = ast.literal_eval(classes)\n",
    "\n",
    "\n",
    "        processed_chunks.append(classes)\n",
    "        total_number = total_number + chunk_size\n",
    "        print(f\"total processed:{total_number}\")\n",
    "#complete_ethnicity = pd.concat(processed_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('./ethnicity_dict_list.pkl', 'wb') as file:\n",
    "    pickle.dump(processed_chunks, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ethnicity_dict_list.pkl', 'rb') as file:\n",
    "    processed_chunks = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kola edward edgal': 8,\n",
       " 'victor gbolade osibodu': 8,\n",
       " 'harrison eyitayo ilori': 8,\n",
       " 'deji alli': 8,\n",
       " 'ishwardas mahtani': 1,\n",
       " 'segun fagboyegun': 8,\n",
       " 'rajan vashdev vaswani': 1,\n",
       " 'gabriel g. boulos': 6,\n",
       " 'olalekan akinsoga akinyanmi': 8,\n",
       " 'ajoritsedere josephine awosika': 8,\n",
       " 'oladipo odujinrin': 8,\n",
       " 'karim elie gabriel boulos': 6,\n",
       " 'gian angelo perrucci': 13,\n",
       " 'naresh asnani': 1,\n",
       " 'hakeem abdul olajuwon': 8,\n",
       " 'si nureni agboola abiola': 8,\n",
       " 'vinay b. mahtani': 1,\n",
       " 'gilbert ramez chagoury': 7,\n",
       " 'suresh murli chellaram': 8,\n",
       " 'oluwagbemiga a. oyebode': 8,\n",
       " 'bhagwan ishwardas mahtani': 1,\n",
       " 'chinedu u. echeruo': 8,\n",
       " 'rita vaswani': 1,\n",
       " 'jason chukwuma njoku': 8,\n",
       " 'bashorun adeniyi adeoye': 8,\n",
       " 'amisha hathiramani': 1,\n",
       " 'adolor uwamu': 8,\n",
       " 'sifawu lawal': 8,\n",
       " 'daisy ehanire danjuma': 8,\n",
       " 'samuel adedoyin': 8,\n",
       " 'ademola benjamin aladekomo': 8,\n",
       " 'olufemi otedola': 8,\n",
       " 'aderemi muyinudeen makanjuola': 8,\n",
       " 'frederick enitiolorunda obateru akinruntan': 8,\n",
       " 'offiong ekanem ejindu': 8,\n",
       " 'ayoola obafunke otudeko': 8,\n",
       " 'yomi folawiyo': 8,\n",
       " 'onajite paul okoloko': 8,\n",
       " 'otunba subomi balogun': 8,\n",
       " 'jimoh ibrahim': 8,\n",
       " 'iyabo adegbemile': 8,\n",
       " 'limota lawal': 8,\n",
       " 'folarin o. alakija': 8,\n",
       " 'nduka obaigbena': 8,\n",
       " 'bashorun jide omokore': 8,\n",
       " 'adedeji tajudeen adeleke': 8,\n",
       " 'adebisi otudeko': 8,\n",
       " 'emmanuel isichei ugochukwu ojei': 8,\n",
       " 'cornelis gerardus vink': 13,\n",
       " 'chukwuemeka emmanuel ndu': 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(processed_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = []\n",
    "for d in processed_chunks:\n",
    "    combined_dict.append(ast.literal_eval(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = {}\n",
    "for d in range(0,len(processed_chunks)):\n",
    "    #print(d)\n",
    "    #some of the dictionaries are truncated and so require parsing with regex to make valid,\n",
    "    #improvements to the classification mapping would be ideal as it would make this stage irrelevant and ensure that all entries have values\n",
    "    pairs = re.findall(r\"'(.*?)': (\\d+)\", processed_chunks[d])\n",
    "    all_valid_dict = {key: int(value) for key, value in pairs}\n",
    "    combined_dict.update(all_valid_dict)\n",
    "\n",
    "#\n",
    "#The code below was the original parser, however it failed for the above reason, it is kept incase the classifier can be fixed\n",
    "#\n",
    "#combined_dict = []\n",
    "#for d in processed_chunks:\n",
    "#    combined_dict.append(ast.literal_eval(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kola edward edgal</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>victor gbolade osibodu</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harrison eyitayo ilori</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deji alli</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ishwardas mahtani</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>phillip allen gamma frost</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>robert jeffrey speyer</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>stephen alan wynn</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>fawaz abdulaziz fahad alhokair</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>john tsung-chen chao</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2016 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                name  ethnicity\n",
       "0                  kola edward edgal          8\n",
       "1             victor gbolade osibodu          8\n",
       "2             harrison eyitayo ilori          8\n",
       "3                          deji alli          8\n",
       "4                  ishwardas mahtani          1\n",
       "...                              ...        ...\n",
       "2011       phillip allen gamma frost          4\n",
       "2012           robert jeffrey speyer          4\n",
       "2013               stephen alan wynn          4\n",
       "2014  fawaz abdulaziz fahad alhokair          7\n",
       "2015            john tsung-chen chao         10\n",
       "\n",
       "[2016 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(combined_dict.items()), columns=['name', 'ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Indian Subcontinent',\n",
       " 2: 'Russia',\n",
       " 3: 'China',\n",
       " 4: 'Anglophone (United States, United Kingdom, Australia, Canada, etc.)',\n",
       " 5: 'Germany',\n",
       " 6: 'Brazil',\n",
       " 7: 'Middle East and North Africa (MENA)',\n",
       " 8: 'Sub-Saharan Africa',\n",
       " 9: 'Latin America (excluding Brazil)',\n",
       " 10: 'East Asia (excluding China)',\n",
       " 11: 'South East Asia',\n",
       " 12: 'Eastern Europe (excluding Russia)',\n",
       " 13: 'Western Europe (excluding Germany and the Anglophone countries)'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[39m=\u001b[39m identify_ethncity_classes(names_ethnicity_df\u001b[39m.\u001b[39;49mloc[\u001b[39m0\u001b[39;49m:\u001b[39m100\u001b[39;49m, :], countries_dict, chunk_size \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m , rate_limit\u001b[39m=\u001b[39;49m\u001b[39m60000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[69], line 49\u001b[0m, in \u001b[0;36midentify_ethncity_classes\u001b[0;34m(df, class_list, chunk_size, rate_limit)\u001b[0m\n\u001b[1;32m     32\u001b[0m pairs_string \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mp[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00mp[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m pairs])\n\u001b[1;32m     34\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[39m            Read the list of country of origin industry classes shown surrounded by 4 colon\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms below\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[39m            ::::\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m            [entry_1, entry_2,.. entry_n]\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[39m            \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m classes, tokens \u001b[39m=\u001b[39m get_model_response(prompt, engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m chunk\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39mclasses\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m classes\n\u001b[1;32m     51\u001b[0m tokens_accumulated \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tokens\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "test = identify_ethncity_classes(names_ethnicity_df.loc[0:100, :], countries_dict, chunk_size = 100 , rate_limit=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identifying industries using cosine similarity\n",
    "\n",
    "This didn't work very well due to the non-linear multi-label relationships. However, I am keeping it in for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cosine_similarity(df_A, df_B):\n",
    "    # Assuming that the embeddings in both dataframes are stored in a column named 'embeddings'\n",
    "    embeddings_A = np.stack(df_A['embedding'].values)\n",
    "    embeddings_B = np.stack(df_B['embedding'].values)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim_matrix = cosine_similarity(embeddings_A, embeddings_B)\n",
    "    \n",
    "    return cosine_sim_matrix\n",
    "\n",
    "\n",
    "def compute_similarity_statistics(cosine_sim_matrix, min = 0, max = 1, num_cutoffs=1000):\n",
    "    cutoffs = np.linspace(min, max, num=num_cutoffs)\n",
    "    \n",
    "    statistics = {\n",
    "        'Cutoff': [],\n",
    "        'Maximum': [],\n",
    "        'Minimum': [],\n",
    "        'Mean': [],\n",
    "        'Median': [],\n",
    "        'Standard Deviation': []\n",
    "    }\n",
    "\n",
    "    for cutoff in cutoffs:\n",
    "        row_sums = np.sum(cosine_sim_matrix >= cutoff, axis=1)\n",
    "        if row_sums.any():\n",
    "            statistics['Cutoff'].append(cutoff)\n",
    "            statistics['Maximum'].append(np.max(row_sums))\n",
    "            statistics['Minimum'].append(np.min(row_sums))\n",
    "            statistics['Mean'].append(np.mean(row_sums))\n",
    "            statistics['Median'].append(np.median(row_sums))\n",
    "            statistics['Standard Deviation'].append(np.std(row_sums))\n",
    "\n",
    "    result_df = pd.DataFrame(statistics)\n",
    "    return result_df\n",
    "\n",
    "test = compute_cosine_similarity(all_bios_df, gics_embeddings_df)\n",
    "\n",
    "test2 = compute_similarity_statistics(test, 0.7, 0.73, num_cutoffs=1000)\n",
    "test2.loc[test2['Minimum']==1].sort_values('Cutoff', ascending=False).head()\n",
    "\n",
    "sns.lineplot(data = test2, x = 'Cutoff', y = 'Minimum')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
